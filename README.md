# llama-70b-2bit-gguf
run llama 70b  in 2bit gguf with gpt4all and llama cpp on cpu colab
